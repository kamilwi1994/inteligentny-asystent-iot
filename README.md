# Inteligentny Asystent IoT (Local LLM + RAG)

Projekt zrealizowany w ramach przedmiotu "Internet Rzeczy". System integruje dane z inteligentnego domu z lokalnym modelem jzykowym (Ollama) przy u偶yciu architektury RAG.

Projekt jest w peni konfigurowalny poprzez plik `.env`.

---

##  Spis treci
1. [Wymagania](#wymagania)
2. [Szybki start](#szybki-start)
3. [Konfiguracja (.env)](#konfiguracja-env)
4. [Architektura Sieciowa](#architektura-sieciowa)

---

##  Wymagania
*   Docker & Docker Compose

---

##  Szybki start

### Krok 1: Pobierz projekt
```bash
git clone https://github.com/kamilwi1994/inteligentny-asystent-iot.git
cd inteligentny-asystent-iot
```

### Krok 2: Konfiguracja
W g贸wnym katalogu znajduje si plik `.env`. Otw贸rz go i dostosuj do swojego rodowiska (domylne wartoci s gotowe do u偶ycia).

**Przykadowa zawarto .env:**
```ini
AI_NETWORK_NAME=ai_network
OLLAMA_BASE_URL=http://ollama:11434
```

### Krok 3: Przygotuj sie Docker
Poniewa偶 aplikacja czy si z zewntrznym kontenerem AI, utw贸rz sie o nazwie zdefiniowanej w `.env`:

```bash
docker network create ai_network
```

### Krok 4: Uruchom Ollam (Silnik AI)
Jeli jeszcze nie masz uruchomionej Ollamy, uruchom j w tej samej sieci:

```bash
docker run -d --name ollama --network ai_network -p 11434:11434 -v ollama_data:/root/.ollama ollama/ollama:latest
```

### Krok 5: Uruchom Aplikacj
```bash
docker compose up --build
```
Aplikacja automatycznie wykryje brakujce modele AI i je pobierze.

Dostp: **http://localhost:8501**

---

## 锔 Konfiguracja (.env)

Plik `.env` pozwala na pen kontrol nad aplikacj bez zmiany kodu:

| Zmienna | Domylnie | Opis |
| :--- | :--- | :--- |
| `AI_NETWORK_NAME` | `ai_network` | Nazwa zewntrznej sieci Docker, w kt贸rej dziaa Ollama. |
| `OLLAMA_BASE_URL` | `http://ollama:11434` | Adres URL kontenera Ollama (zazwyczaj `http://nazwa_kontenera:port`). |
| `OLLAMA_MODEL_CHAT` | `llama3.2` | Model u偶ywany do generowania odpowiedzi. |
| `HISTORY_DAYS` | `30` | Zakres dni generowanych w bazie demo i analizowanych przez RAG. |
| `RAG_K_RETRIEVAL` | `3` | Liczba dni pobierana do kontekstu (im mniej, tym szybciej dziaa na CPU). |

---

##  Architektura Sieciowa

Aplikacja zakada, 偶e Ollama dziaa jako osobny serwis (mikroserwis).
Dziki zmiennej `AI_NETWORK_NAME` w pliku `.env`, mo偶esz atwo podpi asystenta do dowolnej istniejcej infrastruktury Dockerowej, wpisujc po prostu nazw sieci, w kt贸rej dziaa Tw贸j LLM.
```